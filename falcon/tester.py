import argparse
import random
from copy import deepcopy
from tqdm import tqdm

from .falcon_util import *
from .model_editor import *


SEED = 7
torch.manual_seed(SEED)
torch.cuda.manual_seed(SEED)


def load_datas(in_file_path: str):
    in_file = open_file(in_file_path, mode='r')
    datas = json.load(in_file)

    print(f'# data_preprocessor.load_datas() datas size : {len(datas)}, in_file_path : {in_file_path}')
    return datas


def get_model_editor(num_edits=100, hparams_fname_suffix='', hparams_mod=None, identical_num=1, merged_model_path=None, pruning=False):
    alg_name = 'MEMIT'
    model_name = 'gpt2-xl'
    hparams_fname = model_name + '{}.json'.format(hparams_fname_suffix)
    ds_name = 'mcf'
    # num_edits = 100

    dataset_size_limit = None
    continue_from_run = None
    skip_generation_tests = False
    generation_test_interval = 1
    conserve_memory = False
    dir_name = alg_name
    use_cache = False
    output_hidden_states = False

    model_editor = ModelEditor(
        alg_name, model_name, hparams_fname, ds_name,
        dataset_size_limit, continue_from_run, skip_generation_tests,
        generation_test_interval, conserve_memory, dir_name, num_edits, use_cache, output_hidden_states,
        hparams_mod, identical_num, merged_model_path, pruning
    )

    return model_editor


def edit_and_save():
    # í˜„ì¬ ëª©ì : ê° batchë¥¼ ë…ë¦½ì ìœ¼ë¡œ í¸ì§‘í•˜ê³  ëª¨ë¸ë§Œ ì €ì¥ (í‰ê°€ëŠ” ì´í›„ ë³„ë„ ì‹¤í–‰)

    home_dir = '/home/albert0811/dev_env/git/repos/memit'
    data_dir = f'{home_dir}/data/preprocessing'

    for identical_num, num_edits in zip([2], [250]):
        # identical_num: ì´ batchì˜ ê°œìˆ˜
        # num_edits: í•˜ë‚˜ì˜ batchì— ë“¤ì–´ê°„ ë°ì´í„° ê°œìˆ˜

        model_editor = get_model_editor(num_edits)

        model_editor._do_eval_org_model = False
        model_editor._do_eval_new_model = False
        
        # === Normal Edit ===
        size = identical_num * num_edits
        in_file_path = f'{data_dir}/normal/mcf_sampled_{size}.json'
        datas_subject = load_datas(in_file_path)

        # # === Sequential Edit ===
        # in_file_path = f'{data_dir}/mcf_sequential_identical{identical_num}_subjects_all.json'
        # datas_subject = load_datas(in_file_path)


        # # === Multiple Edit ===
        # size = identical_num * num_edits
        # if identical_num == 2:
        #     in_file_path = f'{data_dir}/multi_counterfact_identical{identical_num}_ext_n_{size}.json'
        # else:
        #     in_file_path = f'{data_dir}/multi_counterfact_identical{identical_num}_all_{size}.json'
        # datas_subject = load_datas(in_file_path)


        print("\n[SAVE] ê° batchë³„ë¡œ í¸ì§‘í•œ ëª¨ë¸ ë…ë¦½ì ìœ¼ë¡œ ì €ì¥ì¤‘...")
        model_editor.edit_ext_datas(
            datas_subject,
            do_org_test=False,
            do_edit=True,
            do_edit_test=False,
            do_extend_test=False,
            do_restore=True,
            do_restore_test=False,
            do_print=True,
            do_save=True  # ì €ì¥ ëª©ì 
        )
        
        # Origin Edit method í‰ê°€ ìˆ˜í–‰
        model_editor_org = get_model_editor(num_edits)
        model_editor_org._do_eval_org_model = False
        model_editor_org._do_eval_new_model = False

        print("\n[Test] ê¸°ì¡´ Editing ë°©ë²•ì— ëŒ€í•´ í‰ê°€ ìˆ˜í–‰ ì¤‘...")
        model_editor_org.edit_ext_datas(datas_subject, False, True, False, True, False, False)


def merge_test(model_name):

    # ê²½ë¡œ ì„¤ì •
    home_dir = '/home/albert0811/dev_env/git/repos/memit'
    data_dir = f'{home_dir}/data/preprocessing'
    merged_model_path = Path(f"{home_dir}/merged/{model_name}")

    # ë³‘í•©ëœ ëª¨ë¸ ë””ë ‰í† ë¦¬ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    if not merged_model_path.exists():
        print(f"[Error] ë³‘í•©ëœ ëª¨ë¸ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {merged_model_path}")
        return

    print(f"[Test] ë³‘í•©ëœ ëª¨ë¸ ë¡œë“œ ì¤‘... from {merged_model_path}")
    model = AutoModelForCausalLM.from_pretrained(
        str(merged_model_path),
        torch_dtype=torch.float32
    ).cuda()
    tok = AutoTokenizer.from_pretrained(str(merged_model_path))
    tok.pad_token = tok.eos_token


    # === Normal Edit ===
    # ì‹¤í—˜ìš© ë°ì´í„° ë¡œë“œ ë° í‰ê°€ ì‹¤í–‰
    for identical_num, num_edits in zip([2], [250]):
        size = identical_num * num_edits
        in_file_path = f'{data_dir}/normal/mcf_sampled_{size}.json'
        datas_subject = load_datas(in_file_path)


        # Mergeí•œ ëª¨ë¸ í‰ê°€ ìˆ˜í–‰
        model_editor_merging = get_model_editor(num_edits)
        model_editor_merging._model = model
        model_editor_merging._tok = tok
        model_editor_merging._do_eval_org_model = False
        model_editor_merging._do_eval_new_model = True

        print("[Test] ë³‘í•©ëœ ëª¨ë¸ì— ëŒ€í•´ í‰ê°€ ìˆ˜í–‰ ì¤‘...")
        model_editor_merging.edit_ext_datas(
            datas_subject,
            do_org_test=False,
            do_edit=False,
            do_edit_test=False,
            do_extend_test=True,  # ğŸ”¹ í‰ê°€ ëª©ì 
            do_restore=False,
            do_restore_test=False,
            do_print=False,
            do_save=False
        )
    

    # # === Sequential Edit ===
    # # ì‹¤í—˜ìš© ë°ì´í„° ë¡œë“œ ë° í‰ê°€ ì‹¤í–‰
    # for identical_num, num_edits in zip([4], [5]):
    #     in_file_path = f'{data_dir}/mcf_sequential_identical{identical_num}_subjects_all.json'
    #     datas_subject = load_datas(in_file_path)


    #     # Mergeí•œ ëª¨ë¸ í‰ê°€ ìˆ˜í–‰
    #     model_editor_merging = get_model_editor(num_edits)
    #     model_editor_merging._model = model
    #     model_editor_merging._tok = tok
    #     model_editor_merging._do_eval_org_model = False
    #     model_editor_merging._do_eval_new_model = True

    #     print("[Test] ë³‘í•©ëœ ëª¨ë¸ì— ëŒ€í•´ í‰ê°€ ìˆ˜í–‰ ì¤‘...")
    #     model_editor_merging.edit_ext_datas(
    #         datas_subject,
    #         do_org_test=False,
    #         do_edit=False,
    #         do_edit_test=False,
    #         do_extend_test=True,  # ğŸ”¹ í‰ê°€ ëª©ì 
    #         do_restore=False,
    #         do_restore_test=False,
    #         do_print=False,
    #         do_save=False
    #     )


    # # === Multiple Edit ===
    # # ì‹¤í—˜ìš© ë°ì´í„° ë¡œë“œ ë° í‰ê°€ ì‹¤í–‰
    # for identical_num, num_edits in zip([2], [500]):
    #     size = identical_num * num_edits
    #     if identical_num == 2:
    #         in_file_path = f'{data_dir}/multi_counterfact_identical{identical_num}_ext_n_{size}.json'
    #     else:
    #         in_file_path = f'{data_dir}/multi_counterfact_identical{identical_num}_all_{size}.json'
    #     datas_subject = load_datas(in_file_path)


    #     # Mergedëœ ëª¨ë¸ í‰ê°€ ìˆ˜í–‰
    #     model_editor = get_model_editor(num_edits)
    #     model_editor._model = model
    #     model_editor._tok = tok
    #     model_editor._do_eval_org_model = False
    #     model_editor._do_eval_new_model = True

    #     print("[Test] ë³‘í•©ëœ ëª¨ë¸ì— ëŒ€í•´ í‰ê°€ ìˆ˜í–‰ ì¤‘...")
    #     model_editor.edit_ext_datas(
    #         datas_subject,
    #         do_org_test=False,
    #         do_edit=False,
    #         do_edit_test=False,
    #         do_extend_test=True,  # ğŸ”¹ í‰ê°€ ëª©ì 
    #         do_restore=False,
    #         do_restore_test=False,
    #         do_print=False,
    #         do_save=False
    #     )


def kcc_edit_and_save(identical_nums, num_edits_list):
    home_dir = '/home/albert0811/dev_env/git/repos/memit'
    data_dir = f'{home_dir}/data/preprocessing'

    for identical_num, num_edits in zip(identical_nums, num_edits_list):
        model_editor = get_model_editor(num_edits, identical_num=identical_num)

        model_editor._do_eval_org_model = False
        model_editor._do_eval_new_model = False
        
        size = identical_num * num_edits
        in_file_path = f'{data_dir}/normal/mcf_sampled_{size}.json'
        datas_subject = load_datas(in_file_path)

        print("\n[SAVE] ê° batchë³„ë¡œ ëª¨ë¸ í¸ì§‘ ë° ë…ë¦½ì ìœ¼ë¡œ ì €ì¥ì¤‘...")
        model_editor.edit_ext_datas(
            datas_subject,
            do_org_test=False,
            do_edit=True,
            do_edit_test=True,
            do_extend_test=False,
            do_restore=True,
            do_restore_test=False,
            do_print=True,
            do_save=True,  # ì €ì¥ ëª©ì 
            log_independent=True,
            log_merged=False,
            log_memit=False
        )


def kcc_merge_test(identical_nums, num_edits_list):
    # ê²½ë¡œ ì„¤ì •
    home_dir = '/home/albert0811/dev_env/git/repos/memit'
    data_dir = f'{home_dir}/data/preprocessing'
    merged_dir = Path(f"{home_dir}/merged")

    for identical_num, num_edits in zip(identical_nums, num_edits_list):
        # Origin Edit method í‰ê°€ ìˆ˜í–‰
        model_editor_org = get_model_editor(num_edits, identical_num=identical_num)
        model_editor_org._do_eval_org_model = False
        model_editor_org._do_eval_new_model = False

        size = identical_num * num_edits
        in_file_path = f'{data_dir}/normal/mcf_sampled_{size}.json'
        datas_subject = load_datas(in_file_path)

        # /logs ë””ë ‰í† ë¦¬ì— memit_{identical_num}_{num_edits}.txtì™€ ë™ì¼í•œ íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        log_file_path = f"./logs/memit_{identical_num}_{num_edits}.txt"
        print("#########################")
        print(f"log_file_path: {log_file_path}")
        if os.path.exists(log_file_path):
            print(f"[Warning] ë™ì¼í•œ ì´ë¦„ì˜ ë¡œê·¸ íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {log_file_path}")
            print("ê¸°ì¡´ Editing ë°©ë²•ì— ëŒ€í•œ í‰ê°€ë¥¼ ê±´ë„ˆë›°ê³  ë³‘í•©ëœ ëª¨ë¸ ë””ë ‰í† ë¦¬ íƒìƒ‰ìœ¼ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.")
        else:
            print("\n[Test] ê¸°ì¡´ Editing ë°©ë²•ì— ëŒ€í•´ í‰ê°€ ìˆ˜í–‰ ì¤‘...")
            model_editor_org.edit_ext_datas(
                datas_subject,
                do_org_test=False,
                do_edit=True,
                do_edit_test=True,
                do_extend_test=False,
                do_restore=False,
                do_restore_test=False,
                do_print=True,
                do_save=False,
                log_independent=False,
                log_merged=False,
                log_memit=True
            )

        # ë³‘í•©ëœ ëª¨ë¸ ë””ë ‰í† ë¦¬ íƒìƒ‰
        for merged_model_path in merged_dir.iterdir():
            if not merged_model_path.is_dir():
                continue

            # /logs ë””ë ‰í† ë¦¬ì— ë™ì¼í•œ ì´ë¦„ì˜ txt íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            log_file_path = f"./logs/{merged_model_path.name}.txt"
            print("#########################")
            print(f"log_file_path: {log_file_path}")
            if os.path.exists(log_file_path):
                print(f"[Warn] ë™ì¼í•œ ì´ë¦„ì˜ ë¡œê·¸ íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {log_file_path}")
                print("ë‹¤ìŒ ê²½ë¡œë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.")
                continue

            print(f"[Test] ë³‘í•©ëœ ëª¨ë¸ ë¡œë“œ ì¤‘... from {merged_model_path}")
            model = AutoModelForCausalLM.from_pretrained(
                str(merged_model_path),
                torch_dtype=torch.float32
            ).cuda()
            tok = AutoTokenizer.from_pretrained(str(merged_model_path))
            tok.pad_token = tok.eos_token


            # ëª¨ë¸ í¸ì§‘ê¸° ì´ˆê¸°í™”
            if "gpt2_xl" in str(merged_model_path):
                model_editor_merging = get_model_editor(int(num_edits/2), identical_num=int(identical_num*2), merged_model_path=merged_model_path, pruning=True)
            else:
                model_editor_merging = get_model_editor(num_edits, identical_num=identical_num, merged_model_path=merged_model_path)


            # Mergeí•œ ëª¨ë¸ í‰ê°€ ìˆ˜í–‰
            model_editor_merging._model = model
            model_editor_merging._tok = tok
            model_editor_merging._do_eval_org_model = False
            model_editor_merging._do_eval_new_model = False

            print("[Test] ë³‘í•©ëœ ëª¨ë¸ì— ëŒ€í•´ í‰ê°€ ìˆ˜í–‰ ì¤‘...")
            model_editor_merging.edit_ext_datas(
                datas_subject,
                do_org_test=False,
                do_edit=False,
                do_edit_test=True,
                do_extend_test=False,  # ğŸ”¹ í‰ê°€ ëª©ì 
                do_restore=False,
                do_restore_test=False,
                do_print=True,
                do_save=False,
                log_independent=False,
                log_merged=True,
                log_memit=False
            )


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Process some integers.')
    parser.add_argument('--identical_nums', type=int, nargs='+', help='List of identical numbers for batches')
    parser.add_argument('--num_edits_list', type=int, nargs='+', help='List of number of edits per batch')

    args = parser.parse_args()

    
    # ë…ë¦½ì ìœ¼ë¡œ í¸ì§‘ í›„ ì €ì¥
    # kcc_edit_and_save(args.identical_nums, args.num_edits_list)

    # memit í™˜ê²½ì—ì„œ ì‹¤í–‰
    kcc_merge_test(args.identical_nums, args.num_edits_list)
